{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5cedb4c",
   "metadata": {},
   "source": [
    "## 读取 .env 文件\n",
    "\n",
    "可以使用 `python-dotenv` 库来加载 `.env` 文件中的环境变量。首先确保已经安装了该库 (`pip install python-dotenv`)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "776f22d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: admin\n",
      "Password: admin123\n",
      "Database: vectordb\n",
      "Port: 5432\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# 加载 .env 文件中的环境变量\n",
    "# 这会查找当前目录或父目录中的 .env 文件\n",
    "load_dotenv()\n",
    "\n",
    "# 现在你可以像访问普通环境变量一样访问它们\n",
    "postgres_user = os.getenv('POSTGRES_USER')\n",
    "postgres_password = os.getenv('POSTGRES_PASSWORD')\n",
    "postgres_db = os.getenv('POSTGRES_DB')\n",
    "postgres_port = os.getenv('POSTGRES_PORT')\n",
    "\n",
    "print(f\"User: {postgres_user}\")\n",
    "print(f\"Password: {postgres_password}\")\n",
    "print(f\"Database: {postgres_db}\")\n",
    "print(f\"Port: {postgres_port}\")\n",
    "\n",
    "# 确保你的 .env 文件与 notebook 在同一个目录，或者在父目录中\n",
    "# .env 文件内容示例:\n",
    "# POSTGRES_USER=your_user\n",
    "# POSTGRES_PASSWORD=your_password\n",
    "# POSTGRES_DB=your_database\n",
    "# POSTGRES_PORT=5432"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6547037",
   "metadata": {},
   "source": [
    "### 在 Jupyter Notebook 中运行 Asyncio 代码\n",
    "\n",
    "当你尝试在 Jupyter Notebook 中使用 `asyncio.run()` 时，通常会遇到 `RuntimeError: asyncio.run() cannot be called from a running event loop`。这是因为 Jupyter Notebook (特别是其内核 `ipykernel`) 已经管理着一个正在运行的 `asyncio` 事件循环。\n",
    "\n",
    "`asyncio.run()` 设计为启动一个新的事件循环，这与 Jupyter 已经运行的循环冲突。\n",
    "\n",
    "正确的做法是直接 `await` 你的异步函数。Jupyter/IPython 会在它现有的事件循环中处理这个 `await`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64a73bac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 1, 'name': 'test_name1'}\n",
      "{'id': 2, 'name': 'test_name2'}\n",
      "{'id': 3, 'name': 'test_name1'}\n",
      "{'id': 4, 'name': 'test_name2'}\n",
      "{'id': 5, 'name': 'test_name1'}\n",
      "{'id': 6, 'name': 'test_name2'}\n",
      "{'id': 7, 'name': 'test_name1'}\n",
      "{'id': 8, 'name': 'test_name2'}\n",
      "{'id': 9, 'name': 'test_name1'}\n",
      "{'id': 10, 'name': 'test_name2'}\n",
      "{'id': 11, 'name': 'test_name1'}\n",
      "{'id': 12, 'name': 'test_name2'}\n",
      "{'id': 13, 'name': 'test_name1'}\n",
      "{'id': 14, 'name': 'test_name2'}\n"
     ]
    }
   ],
   "source": [
    "import asyncpg\n",
    "\n",
    "# 建立连接\n",
    "conn = await asyncpg.connect(\n",
    "    user=postgres_user,\n",
    "    password=postgres_password,\n",
    "    database=postgres_db,\n",
    "    host='127.0.0.1',\n",
    "    port=postgres_port # 确保也使用了 .env 中的端口\n",
    ")\n",
    "\n",
    "\n",
    "# 执行查询\n",
    "# 示例：创建一个表（如果它不存在）并插入一些数据\n",
    "await conn.execute('''\n",
    "    CREATE TABLE IF NOT EXISTS your_table (\n",
    "        id SERIAL PRIMARY KEY,\n",
    "        name TEXT\n",
    "    );\n",
    "''')\n",
    "await conn.execute(\"INSERT INTO your_table (name) VALUES ($1), ($2)\", \"test_name1\", \"test_name2\")\n",
    "\n",
    "rows = await conn.fetch('SELECT * FROM your_table')\n",
    "\n",
    "# 打印结果\n",
    "for row in rows:\n",
    "    print(dict(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "956c9b14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Record table_name='your_table'>\n",
      "<Record table_name='testvector'>\n",
      "<Record table_name='arxiv_meta'>\n"
     ]
    }
   ],
   "source": [
    "# show all tables\n",
    "tables = await conn.fetch('''\n",
    "    SELECT table_name\n",
    "    FROM information_schema.tables\n",
    "    WHERE table_schema = 'public';\n",
    "''')\n",
    "for table in tables:\n",
    "    print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ba43724",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Record extname='plpgsql' extversion='1.0'>,\n",
       " <Record extname='vector' extversion='0.8.0'>,\n",
       " <Record extname='vchord' extversion='0.3.0'>]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pgvector.asyncpg import register_vector\n",
    "await conn.execute(\"CREATE EXTENSION IF NOT EXISTS vector;\")\n",
    "await conn.execute(\"CREATE EXTENSION IF NOT EXISTS vchord CASCADE;\")\n",
    "extensions = await conn.fetch('SELECT extname, extversion FROM pg_extension;')\n",
    "await register_vector(conn)\n",
    "extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c6343bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table \"testvector\" dropped if it existed, before recreation.\n"
     ]
    }
   ],
   "source": [
    "await conn.execute('DROP TABLE IF EXISTS testvector CASCADE;')\n",
    "await conn.execute('DROP TABLE IF EXISTS arxiv_meta CASCADE;')\n",
    "print('Table \"testvector\" dropped if it existed, before recreation.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b256240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'test_id_pgvector', 'content': 'test_content_pgvector', 'embedding': HalfVector([0.0999755859375, 0.199951171875, 0.300048828125, 0.39990234375])}\n",
      ">f2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 创建一个表，testvector，使用4维度的float16 向量，有3个字段，id(手动指定的字符串)，content，embedding，如果不存在\n",
    "await conn.execute('''\n",
    "    CREATE TABLE IF NOT EXISTS testvector (\n",
    "        id TEXT PRIMARY KEY,\n",
    "        content TEXT,\n",
    "        embedding HALFVEC(4)\n",
    "    );\n",
    "''')\n",
    "# 插入数据\n",
    "\n",
    "# 定义一个 NumPy 数组\n",
    "vector_data_np = np.array([0.1, 0.2, 0.3, 0.4], dtype=np.float16)\n",
    "\n",
    "# With pgvector.asyncpg and register_vector(conn), \n",
    "# we should be able to pass the numpy array directly.\n",
    "await conn.execute('''\n",
    "    INSERT INTO testvector (id, content, embedding)\n",
    "    VALUES ($1, $2, $3)\n",
    "    ON CONFLICT (id) DO NOTHING;\n",
    "''', 'test_id_pgvector', 'test_content_pgvector', vector_data_np)\n",
    "\n",
    "# 查询数据\n",
    "rows = await conn.fetch('SELECT * FROM testvector WHERE id = $1', 'test_id_pgvector')\n",
    "# 打印结果  \n",
    "for row in rows:\n",
    "    print(dict(row))\n",
    "    print(row['embedding'].to_numpy().dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe21d3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Record table_name='your_table'>\n",
      "<Record table_name='testvector'>\n",
      "<Record table_name='arxiv_meta'>\n"
     ]
    }
   ],
   "source": [
    "# Schema({'id': String, 'title': String, 'authors': List(String), 'abstract': String, 'date': String, 'categories': List(String), 'created': String, 'updated': String, 'license': String, 'jasper_v1': Array(Float32, shape=(1024,)), 'conan_v1': Array(Float32, shape=(1792,))})\n",
    "await conn.execute('''\n",
    "    CREATE TABLE IF NOT EXISTS arxiv_meta (\n",
    "        id TEXT PRIMARY KEY,\n",
    "        title TEXT,\n",
    "        authors TEXT[],\n",
    "        abstract TEXT,\n",
    "        date DATE,\n",
    "        categories TEXT[],\n",
    "        created DATE,\n",
    "        updated DATE,\n",
    "        license TEXT,\n",
    "        jasper_v1 halfvec(1024),\n",
    "        conan_v1 halfvec(1792)\n",
    "    );\n",
    "''')\n",
    "\n",
    "# show all tables\n",
    "tables = await conn.fetch('''\n",
    "    SELECT table_name\n",
    "    FROM information_schema.tables\n",
    "    WHERE table_schema = 'public';\n",
    "''')\n",
    "for table in tables:\n",
    "    print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1e23f79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'column_name': 'created', 'data_type': 'date', 'character_maximum_length': None, 'column_default': None, 'is_nullable': 'YES'}\n",
      "{'column_name': 'updated', 'data_type': 'date', 'character_maximum_length': None, 'column_default': None, 'is_nullable': 'YES'}\n",
      "{'column_name': 'date', 'data_type': 'date', 'character_maximum_length': None, 'column_default': None, 'is_nullable': 'YES'}\n",
      "{'column_name': 'jasper_v1', 'data_type': 'USER-DEFINED', 'character_maximum_length': None, 'column_default': None, 'is_nullable': 'YES'}\n",
      "{'column_name': 'conan_v1', 'data_type': 'USER-DEFINED', 'character_maximum_length': None, 'column_default': None, 'is_nullable': 'YES'}\n",
      "{'column_name': 'license', 'data_type': 'text', 'character_maximum_length': None, 'column_default': None, 'is_nullable': 'YES'}\n",
      "{'column_name': 'categories', 'data_type': 'ARRAY', 'character_maximum_length': None, 'column_default': None, 'is_nullable': 'YES'}\n",
      "{'column_name': 'title', 'data_type': 'text', 'character_maximum_length': None, 'column_default': None, 'is_nullable': 'YES'}\n",
      "{'column_name': 'authors', 'data_type': 'ARRAY', 'character_maximum_length': None, 'column_default': None, 'is_nullable': 'YES'}\n",
      "{'column_name': 'abstract', 'data_type': 'text', 'character_maximum_length': None, 'column_default': None, 'is_nullable': 'YES'}\n",
      "{'column_name': 'id', 'data_type': 'text', 'character_maximum_length': None, 'column_default': None, 'is_nullable': 'NO'}\n"
     ]
    }
   ],
   "source": [
    "# 获取 arxiv_meta 表的 schema\n",
    "table_schema = await conn.fetch(\"\"\"\n",
    "    SELECT column_name, data_type, character_maximum_length, column_default, is_nullable\n",
    "    FROM information_schema.columns\n",
    "    WHERE table_name = 'arxiv_meta';\n",
    "\"\"\")\n",
    "\n",
    "for column in table_schema:\n",
    "    print(dict(column))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d749a24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 699547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [01:09<00:00,  9.95s/it]\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "import asyncpg\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 定义公共列和嵌入列\n",
    "common_columns = ['id', 'title', 'authors', 'abstract', 'date', 'categories', 'created', 'updated', 'license']\n",
    "embedding_columns = ['jasper_v1', 'conan_v1']\n",
    "\n",
    "# 定义批次大小\n",
    "batch_size = 100_000\n",
    "lazy_frame = pl.scan_parquet(\"./data/*.parquet\")\n",
    "# 计算总行数\n",
    "total_rows = int(lazy_frame.select(pl.len()).collect().item())\n",
    "print(f\"Total rows: {total_rows}\")\n",
    "\n",
    "date_cols_to_convert = ['date', 'created', 'updated']\n",
    "\n",
    "# 连接到 PostgreSQL 数据库\n",
    "vector_type = np.float16\n",
    "async def main():\n",
    "    # 按批次循环处理数据\n",
    "    for i in tqdm(range(0, total_rows, batch_size)):\n",
    "        # 提取批次\n",
    "        batch_lf = lazy_frame.slice(i, batch_size)\n",
    "        embeddings = {\n",
    "            name: pl.Series(\n",
    "                (batch_lf.select(name)).collect()\n",
    "            ).to_numpy().astype(vector_type)\n",
    "            for name in embedding_columns\n",
    "        }\n",
    "        # 转换日期列\n",
    "        for col_name in date_cols_to_convert:\n",
    "            batch_lf = batch_lf.with_columns(\n",
    "                pl.col(col_name).str.to_date(format=\"%Y-%m-%d\", strict=False).alias(col_name)\n",
    "            )\n",
    "        batch_df = batch_lf.select(common_columns).collect()\n",
    "        values = (\n",
    "            (*item, *[embeddings[name][i] for name in embedding_columns])\n",
    "            for i, item in enumerate(\n",
    "                batch_df.select(common_columns).iter_rows()\n",
    "            )\n",
    "        )\n",
    "        insert_meta_sql = '''\n",
    "            INSERT INTO arxiv_meta (id, title, authors, abstract, date, categories, created, updated, license, jasper_v1, conan_v1)\n",
    "            VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11)\n",
    "            ON CONFLICT (id) DO NOTHING;\n",
    "        '''\n",
    "        await conn.executemany(insert_meta_sql, values)\n",
    "\n",
    "# 运行主函数\n",
    "await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0058127c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows in arxiv_meta: 697495\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 查询 arxiv_meta 表中的数据总数\n",
    "count_result = await conn.fetchval('SELECT COUNT(*) FROM arxiv_meta;')\n",
    "print(f'Total rows in arxiv_meta: {count_result}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a778772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '0704.3649', 'title': 'Quantile and Probability Curves Without Crossing', 'authors': ['Victor Chernozhukov', 'Ivan Fernandez-Val', 'Alfred Galichon'], 'abstract': 'This paper proposes a method to address the longstanding problem of lack of monotonicity in estimation of conditional and structural quantile functions, also known as the quantile crossing problem. The method consists in sorting or monotone rearranging the original estimated non-monotone curve into a monotone rearranged curve. We show that the rearranged curve is closer to the true quantile curve in finite samples than the original curve, establish a functional delta method for rearrangement-related operators, and derive functional limit theory for the entire rearranged curve and its functionals. We also establish validity of the bootstrap for estimating the limit law of the the entire rearranged curve and its functionals. Our limit results are generic in that they apply to every estimator of a monotone econometric function, provided that the estimator satisfies a functional central limit theorem and the function satisfies some smoothness conditions. Consequently, our results apply to estimation of other econometric functions with monotonicity restrictions, such as demand, production, distribution, and structural distribution functions. We illustrate the results with an application to estimation of structural quantile functions using data on Vietnam veteran status and earnings.', 'date': datetime.date(2017, 10, 4), 'categories': ['stat.ME', 'econ.EM', 'math.ST', 'stat.TH'], 'created': datetime.date(2014, 7, 15), 'updated': datetime.date(2017, 10, 4), 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'jasper_v1': array([ 0.00145791, -0.05719407,  0.03682215, ..., -0.03774695,\n",
      "       -0.00282768,  0.02185524], shape=(1024,), dtype=float32), 'conan_v1': array([-0.02020478,  0.01626317, -0.01343192, ..., -0.01547568,\n",
      "       -0.03136969, -0.02172536], shape=(1792,), dtype=float32)}\n",
      "{'id': '0704.3686', 'title': 'Improving Estimates of Monotone Functions by Rearrangement', 'authors': ['Victor Chernozhukov', 'Ivan Fernandez-Val', 'Alfred Galichon'], 'abstract': 'Suppose that a target function is monotonic, namely, weakly increasing, and an original estimate of the target function is available, which is not weakly increasing. Many common estimation methods used in statistics produce such estimates. We show that these estimates can always be improved with no harm using rearrangement techniques: The rearrangement methods, univariate and multivariate, transform the original estimate to a monotonic estimate, and the resulting estimate is closer to the true curve in common metrics than the original estimate. We illustrate the results with a computational example and an empirical example dealing with age-height growth charts.', 'date': datetime.date(2017, 11, 23), 'categories': ['stat.ME', 'econ.EM', 'math.ST', 'stat.TH'], 'created': datetime.date(2010, 11, 3), 'updated': datetime.date(2017, 11, 23), 'license': None, 'jasper_v1': array([ 0.00970491, -0.0304562 ,  0.03659501, ..., -0.0141701 ,\n",
      "        0.02253588,  0.01398511], shape=(1024,), dtype=float32), 'conan_v1': array([-0.03148203,  0.00318344, -0.00853935, ..., -0.01811432,\n",
      "       -0.03069574, -0.02533844], shape=(1792,), dtype=float32)}\n",
      "{'id': '0706.1688', 'title': 'Continuation of connecting orbits in 3D-ODEs: (I) Point-to-cycle connections', 'authors': ['E. J. Doedel', 'B. W. Kooi', 'Yu. A. Kuznetsov', 'G. A. K. van Voorn'], 'abstract': \"We propose new methods for the numerical continuation of point-to-cycle connecting orbits in 3-dimensional autonomous ODE's using projection boundary conditions. In our approach, the projection boundary conditions near the cycle are formulated using an eigenfunction of the associated adjoint variational equation, avoiding costly and numerically unstable computations of the monodromy matrix. The equations for the eigenfunction are included in the defining boundary-value problem, allowing a straightforward implementation in AUTO, in which only the standard features of the software are employed. Homotopy methods to find connecting orbits are discussed in general and illustrated with several examples, including the Lorenz equations. Complete AUTO demos, which can be easily adapted to any autonomous 3-dimensional ODE system, are freely available.\", 'date': datetime.date(2017, 12, 11), 'categories': ['math.DS', 'math.NA'], 'created': datetime.date(2007, 12, 11), 'updated': datetime.date(2017, 12, 11), 'license': None, 'jasper_v1': array([ 0.00863968,  0.00377673,  0.00186731, ...,  0.02219026,\n",
      "       -0.03819603,  0.01243492], shape=(1024,), dtype=float32), 'conan_v1': array([-0.02202029, -0.0012877 ,  0.01004132, ..., -0.01588867,\n",
      "       -0.03425977,  0.00272202], shape=(1792,), dtype=float32)}\n",
      "{'id': '0706.3434', 'title': 'Separating populations with wide data: A spectral analysis', 'authors': ['Avrim Blum', 'Amin Coja-Oghlan', 'Alan Frieze', 'Shuheng Zhou'], 'abstract': 'In this paper, we consider the problem of partitioning a small data sample drawn from a mixture of $k$ product distributions. We are interested in the case that individual features are of low average quality $γ$, and we want to use as few of them as possible to correctly partition the sample. We analyze a spectral technique that is able to approximately optimize the total data size--the product of number of data points $n$ and the number of features $K$--needed to correctly perform this partitioning as a function of $1/γ$ for $K>n$. Our goal is motivated by an application in clustering individuals according to their population of origin using markers, when the divergence between any two of the populations is small.', 'date': datetime.date(2017, 11, 17), 'categories': ['stat.ML', 'stat.AP'], 'created': datetime.date(2009, 1, 29), 'updated': datetime.date(2017, 11, 17), 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'jasper_v1': array([ 0.0717552 ,  0.01137878, -0.00091128, ..., -0.02057115,\n",
      "        0.05115572,  0.0102307 ], shape=(1024,), dtype=float32), 'conan_v1': array([-0.02168552,  0.00350872, -0.00734665, ..., -0.05180412,\n",
      "       -0.00765164, -0.04287516], shape=(1792,), dtype=float32)}\n",
      "{'id': '0708.0169', 'title': 'Data-driven goodness-of-fit tests', 'authors': ['Mikhail Langovoy'], 'abstract': \"We propose and study a general method for construction of consistent statistical tests on the basis of possibly indirect, corrupted, or partially available observations. The class of tests devised in the paper contains Neyman's smooth tests, data-driven score tests, and some types of multi-sample tests as basic examples. Our tests are data-driven and are additionally incorporated with model selection rules. The method allows to use a wide class of model selection rules that are based on the penalization idea. In particular, many of the optimal penalties, derived in statistical literature, can be used in our tests. We establish the behavior of model selection rules and data-driven tests under both the null hypothesis and the alternative hypothesis, derive an explicit detectability rule for alternative hypotheses, and prove a master consistency theorem for the tests from the class. The paper shows that the tests are applicable to a wide range of problems, including hypothesis testing in statistical inverse problems, multi-sample problems, and nonparametric hypothesis testing.\", 'date': datetime.date(2017, 9, 22), 'categories': ['math.ST', 'math.PR', 'stat.ME', 'stat.TH'], 'created': datetime.date(2017, 9, 21), 'updated': datetime.date(2017, 9, 22), 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'jasper_v1': array([ 4.3311268e-02,  5.3373966e-03, -1.7294316e-02, ...,\n",
      "       -8.6550041e-05, -2.6556445e-02,  4.6864931e-02],\n",
      "      shape=(1024,), dtype=float32), 'conan_v1': array([-0.03059303, -0.01214519, -0.0316012 , ..., -0.01938261,\n",
      "       -0.03137843, -0.02050874], shape=(1792,), dtype=float32)}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 查询 arxiv_meta 表的头部数据\n",
    "head_rows = await conn.fetch('SELECT * FROM arxiv_meta LIMIT 5;')\n",
    "for row in head_rows:\n",
    "    print(dict(row))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cdf3c8b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SET'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await conn.execute(\"SET maintenance_work_mem = '16GB';\")\n",
    "await conn.execute(\"SET max_parallel_maintenance_workers = 15;\")\n",
    "await conn.execute(\"SET max_parallel_workers = 15;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7feb0dc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating vchordrq index for jasper_v1...\n",
      "Creating vchordrq index for conan_v1...\n"
     ]
    }
   ],
   "source": [
    "sql_build_index = '''\n",
    "CREATE INDEX IF NOT EXISTS arxiv_meta_{column}_idx \n",
    "ON arxiv_meta \n",
    "USING vchordrq ({column} {ops})\n",
    "WITH (options = $$\n",
    "[build.internal]\n",
    "lists = [{lists}]\n",
    "build_threads = {threads}\n",
    "$$);\n",
    "'''\n",
    "lists = int(2 * total_rows / 1000)\n",
    "threads = 15\n",
    "\n",
    "for column in embedding_columns:\n",
    "    print(f\"Creating vchordrq index for {column}...\")\n",
    "    await conn.execute(sql_build_index.format(\n",
    "        column=column, lists=lists, threads=threads, ops='halfvec_ip_ops'\n",
    "    ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
